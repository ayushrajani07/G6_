#!/usr/bin/env python
"""Generate an inventory of all G6_* environment variable tokens referenced in the repo.

Outputs a machine & human friendly markdown file (default: docs/ENV_VARS_AUTO.md) containing:
  * Summary counts
  * Variables referenced in code but NOT documented in docs/env_dict.md (if any)
  * Variables documented but not referenced in code (potentially stale / historical)
  * Full sorted list of all discovered tokens with first-seen path

Optionally also emits a JSON file (parallel structure) when --json-out is provided (default: docs/ENV_VARS_AUTO.json) for CI / tests.

This script is intentionally lightweight (no external deps). It complements, but does not
replace, the authoritative manually curated `docs/env_dict.md` which contains descriptions.

Usage:
  python scripts/gen_env_inventory.py                # write default output
  python scripts/gen_env_inventory.py --out docs/ENV_VARS_AUTO.md

Exit codes:
  0 success (even if mismatches found)
  2 severe error (I/O issues)

Implementation notes:
  * We scan *.py, *.md, *.ps1, *.sh, *.yml to catch references in workflow & docs.
  * A token is any word boundary match of pattern G6_[A-Z0-9_]+.
  * Duplicate references are collapsed; first occurrence path retained for traceability.
  * The script is safe to run repeatedly (output file overwritten atomically via temp + replace).
"""
from __future__ import annotations

import argparse
import re
import sys
import tempfile
from collections.abc import Iterable
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

TOKEN_RE = re.compile(r"\bG6_[A-Z0-9_]+\b")
DOC_FILE_CANDIDATE = Path("docs/env_dict.md")
DEFAULT_OUT = Path("docs/ENV_VARS_AUTO.md")
DEFAULT_JSON_OUT = Path("docs/ENV_VARS_AUTO.json")
SCAN_EXTENSIONS = {".py", ".md", ".ps1", ".sh", ".yml", ".yaml"}
EXCLUDE_DIRS = {".git", "__pycache__", "archive", "logs", "data", "parity_snapshots"}


@dataclass
class TokenInfo:
    name: str
    first_path: Path
    count: int = 0


def iter_files(root: Path) -> Iterable[Path]:
    for p in root.rglob("*"):
        if not p.is_file():
            continue
        if p.suffix.lower() not in SCAN_EXTENSIONS:
            continue
        # Skip large or generated dirs
        if any(part in EXCLUDE_DIRS for part in p.parts):
            continue
        yield p


def collect_tokens(root: Path) -> dict[str, TokenInfo]:
    found: dict[str, TokenInfo] = {}
    for file_path in iter_files(root):
        try:
            text = file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        for m in TOKEN_RE.finditer(text):
            tok = m.group(0)
            info = found.get(tok)
            if info is None:
                found[tok] = TokenInfo(name=tok, first_path=file_path, count=1)
            else:
                info.count += 1
    return found


def collect_documented(doc_file: Path) -> set[str]:
    if not doc_file.exists():
        return set()
    try:
        text = doc_file.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return set()
    return set(TOKEN_RE.findall(text))


def format_section(header: str, lines: list[str]) -> str:
    out = [f"## {header}", ""]
    if not lines:
        out.append("(none)")
    else:
        out.extend(lines)
    out.append("")
    return "\n".join(out)


def write_output(
    out_file: Path,
    discovered: dict[str, TokenInfo],
    documented: set[str],
    json_out: Path | None = None,
) -> None:
    code_tokens = set(discovered.keys())
    missing_doc = sorted(code_tokens - documented)
    stale_doc = sorted(documented - code_tokens)
    now = datetime.now(UTC).isoformat().replace("+00:00", "Z")

    lines: list[str] = []
    lines.append("# Auto-Generated Environment Variable Inventory")
    lines.append("")
    lines.append(f"Generated: {now}")
    lines.append(
        "This file is generated by `scripts/gen_env_inventory.py` and is informational only. "
        "The authoritative descriptions live in `docs/env_dict.md`."
    )
    lines.append("")
    lines.append("### Summary")
    lines.append("")
    lines.append(f"* Referenced in code: {len(code_tokens)}")
    lines.append(f"* Documented in env_dict.md: {len(documented)}")
    lines.append(f"* Undocumented (need adding to env_dict.md): {len(missing_doc)}")
    lines.append(f"* Potentially stale (documented but not referenced): {len(stale_doc)}")
    lines.append("")

    if missing_doc:
        lines.append(format_section("Undocumented Variables", [f"- {v}" for v in missing_doc]))
    else:
        lines.append(format_section("Undocumented Variables", []))

    if stale_doc:
        lines.append(format_section("Possibly Stale Documented Variables", [f"- {v}" for v in stale_doc]))
    else:
        lines.append(format_section("Possibly Stale Documented Variables", []))

    # Full inventory
    inventory_lines = []
    for name in sorted(code_tokens):
        info = discovered[name]
        inventory_lines.append(
            f"- {name} (refs={info.count}, first={info.first_path.as_posix()})"
        )
    lines.append(format_section("Full Inventory", inventory_lines))

    # Atomic write (markdown)
    out_file.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tf:
        tmp_path = Path(tf.name)
        tf.write("\n".join(lines).rstrip() + "\n")
    tmp_path.replace(out_file)

    if json_out is not None:
        import json
        json_payload: dict[str, Any] = {
            "generated": now,
            "counts": {
                "code_referenced": len(code_tokens),
                "documented": len(documented),
                "undocumented": len(missing_doc),
                "stale": len(stale_doc),
            },
            "undocumented": missing_doc,
            "stale": stale_doc,
            "inventory": [
                {
                    "name": name,
                    "refs": discovered[name].count,
                    "first_path": str(discovered[name].first_path),
                }
                for name in sorted(code_tokens)
            ],
        }
        with tempfile.NamedTemporaryFile("w", delete=False, encoding="utf-8") as tf:
            tmp_json = Path(tf.name)
            json.dump(json_payload, tf, indent=2, sort_keys=True)
            tf.write("\n")
        json_out.parent.mkdir(parents=True, exist_ok=True)
        tmp_json.replace(json_out)


def main(argv: list[str]) -> int:
    ap = argparse.ArgumentParser(description="Generate G6_* env var inventory")
    ap.add_argument("--out", default=str(DEFAULT_OUT), help="Output markdown file path")
    ap.add_argument(
        "--root", default=str(Path(__file__).resolve().parent.parent), help="Project root (auto)"
    )
    ap.add_argument(
        "--json-out", default=str(DEFAULT_JSON_OUT), help="Optional JSON output path (empty to disable)"
    )
    args = ap.parse_args(argv)

    root = Path(args.root).resolve()
    out_path = Path(args.out)
    json_path: Path | None = None
    if getattr(args, "json_out", None):
        jp = str(args.json_out).strip()
        if jp:
            json_path = Path(jp)
    try:
        discovered = collect_tokens(root)
        documented = collect_documented(DOC_FILE_CANDIDATE)
        write_output(out_path, discovered, documented, json_out=json_path)
    except Exception as e:  # pragma: no cover - defensive
        print(f"FATAL: {e}", file=sys.stderr)
        return 2
    msg = f"Inventory written to {out_path} (referenced={len(discovered)} documented={len(documented)})"
    if json_path:
        msg += f" json={json_path}"
    print(msg)
    if not collect_documented(DOC_FILE_CANDIDATE):
        print("NOTE: docs/env_dict.md not found or unreadable; only raw token list produced")
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main(sys.argv[1:]))
